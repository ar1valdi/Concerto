@article{IR_embeddings,
  author       = {Bhaskar Mitra and
                  Nick Craswell},
  title        = {Neural Models for Information Retrieval},
  journal      = {CoRR},
  volume       = {abs/1705.01509},
  year         = {2017},
  url          = {http://arxiv.org/abs/1705.01509},
  eprinttype    = {arXiv},
  eprint       = {1705.01509},
  timestamp    = {Wed, 27 Apr 2022 14:24:37 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/MitraC17.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{Yang,
  author  = {P. Yang and H. Wang and J. Yang and Z. Qian and Y. Zhang and X. Lin},
  title   = {Deep Learning Approaches for Similarity Computation: A Survey},
  journal = {IEEE Transactions on Knowledge and Data Engineering},
  volume  = {36},
  number  = {12},
  pages   = {7893--7912},
  year    = {2024},
  month   = {Dec.}
}

@ARTICLE{Yang_2024,
  author={Yang, Peilun and Wang, Hanchen and Yang, Jianye and Qian, Zhengping and Zhang, Ying and Lin, Xuemin},
  journal={IEEE Transactions on Knowledge and Data Engineering}, 
  title={Deep Learning Approaches for Similarity Computation: A Survey}, 
  year={2024},
  volume={36},
  number={12},
  pages={7893-7912},
  keywords={Measurement;Costs;Task analysis;Vectors;Learning systems;Computational modeling;Search problems;Deep learning;metric learning;similarity computation;data representation},
  doi={10.1109/TKDE.2024.3422484}}

@article{Mueller_Thyagarajan_2016, title={Siamese Recurrent Architectures for Learning Sentence Similarity}, volume={30}, url={https://ojs.aaai.org/index.php/AAAI/article/view/10350}, DOI={10.1609/aaai.v30i1.10350}, abstractNote={ &lt;p&gt; We present a siamese adaptation of the Long Short-Term Memory (LSTM) network for labeled data comprised of pairs of variable-length sequences. Our model is applied to assess semantic similarity between sentences, where we exceed state of the art, outperforming carefully handcrafted features and recently proposed neural network systems of greater complexity. For these applications, we provide word-embedding vectors supplemented with synonymic information to the LSTMs, which use a fixed size vector to encode the underlying meaning expressed in a sentence (irrespective of the particular wording/syntax). By restricting subsequent operations to rely on a simple Manhattan metric, we compel the sentence representations learned by our model to form a highly structured space whose geometry reflects complex semantic relationships. Our results are the latest in a line of findings that showcase LSTMs as powerful language models capable of tasks requiring intricate understanding. &lt;/p&gt; }, number={1}, journal={Proceedings of the AAAI Conference on Artificial Intelligence}, author={Mueller, Jonas and Thyagarajan, Aditya}, year={2016}, month={Mar.} }

@inproceedings{marelli2014semeval,
  title={Semeval-2014 task 1: Evaluation of compositional distributional semantic models on full sentences through semantic relatedness and textual entailment},
  author={Marelli, Marco and Bentivogli, Luisa and Baroni, Marco and Bernardi, Raffaella and Menini, Stefano and Zamparelli, Roberto},
  booktitle={Proceedings of the 8th international workshop on semantic evaluation (SemEval 2014)},
  pages={1--8},
  year={2014}
}

@article{miller1995wordnet,
  title={WordNet: a lexical database for English},
  author={Miller, George A},
  journal={Communications of the ACM},
  volume={38},
  number={11},
  pages={39--41},
  year={1995},
  publisher={ACM New York, NY, USA}
}

@misc{wordembeddingssurvey,
      title={Word Embeddings: A Survey}, 
      author={Felipe Almeida and Geraldo Xexéo},
      year={2023},
      eprint={1901.09069},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1901.09069}, 
}

@article{deerwester1990indexing,
  title={Indexing by latent semantic analysis},
  author={Deerwester, Scott and Dumais, Susan T and Furnas, George W and Landauer, Thomas K and Harshman, Richard},
  journal={Journal of the American society for information science},
  volume={41},
  number={6},
  pages={391--407},
  year={1990},
  publisher={Wiley Online Library}
}

@Article{szymanski,
AUTHOR = {Szymański, Julian and Operlejn, Maksymilian and Weichbroth, Paweł},
TITLE = {Enhancing Word Embeddings for Improved Semantic Alignment},
JOURNAL = {Applied Sciences},
VOLUME = {14},
YEAR = {2024},
NUMBER = {24},
ARTICLE-NUMBER = {11519},
URL = {https://www.mdpi.com/2076-3417/14/24/11519},
ISSN = {2076-3417},
ABSTRACT = {This study introduces a method for the improvement of word vectors, addressing the limitations of traditional approaches like Word2Vec or GloVe through introducing into embeddings richer semantic properties. Our approach leverages supervised learning methods, with shifts in vectors in the representation space enhancing the quality of word embeddings. This ensures better alignment with semantic reference resources, such as WordNet. The effectiveness of the method has been demonstrated through the application of modified embeddings to text classification and clustering. We also show how our method influences document class distributions, visualized through PCA projections. By comparing our results with state-of-the-art approaches and achieving better accuracy, we confirm the effectiveness of the proposed method. The results underscore the potential of adaptive embeddings to improve both the accuracy and efficiency of semantic analysis across a range of NLP.},
DOI = {10.3390/app142411519}
}
@misc{word2vec,
      title={Efficient Estimation of Word Representations in Vector Space}, 
      author={Tomas Mikolov and Kai Chen and Greg Corrado and Jeffrey Dean},
      year={2013},
      eprint={1301.3781},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1301.3781}, 
}
@inproceedings{2014-glove,
    title = "{G}lo{V}e: Global Vectors for Word Representation",
    author = "Pennington, Jeffrey  and
      Socher, Richard  and
      Manning, Christopher",
    editor = "Moschitti, Alessandro  and
      Pang, Bo  and
      Daelemans, Walter",
    booktitle = "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing ({EMNLP})",
    month = oct,
    year = "2014",
    address = "Doha, Qatar",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D14-1162/",
    doi = "10.3115/v1/D14-1162",
    pages = "1532--1543"
}
@inproceedings{glove,
  title={Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP)},
  author={Moschitti, Alessandro and Pang, Bo and Daelemans, Walter},
  booktitle={Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  year={2014}
}
@inproceedings{elmo,
    title = "Deep Contextualized Word Representations",
    author = "Peters, Matthew E.  and
      Neumann, Mark  and
      Iyyer, Mohit  and
      Gardner, Matt  and
      Clark, Christopher  and
      Lee, Kenton  and
      Zettlemoyer, Luke",
    editor = "Walker, Marilyn  and
      Ji, Heng  and
      Stent, Amanda",
    booktitle = "Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)",
    month = jun,
    year = "2018",
    address = "New Orleans, Louisiana",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N18-1202/",
    doi = "10.18653/v1/N18-1202",
    pages = "2227--2237",
    abstract = "We introduce a new type of deep contextualized word representation that models both (1) complex characteristics of word use (e.g., syntax and semantics), and (2) how these uses vary across linguistic contexts (i.e., to model polysemy). Our word vectors are learned functions of the internal states of a deep bidirectional language model (biLM), which is pre-trained on a large text corpus. We show that these representations can be easily added to existing models and significantly improve the state of the art across six challenging NLP problems, including question answering, textual entailment and sentiment analysis. We also present an analysis showing that exposing the deep internals of the pre-trained network is crucial, allowing downstream models to mix different types of semi-supervision signals."
}
@inproceedings{2019-bert,
    title = "{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding",
    author = "Devlin, Jacob  and
      Chang, Ming-Wei  and
      Lee, Kenton  and
      Toutanova, Kristina",
    editor = "Burstein, Jill  and
      Doran, Christy  and
      Solorio, Thamar",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N19-1423/",
    doi = "10.18653/v1/N19-1423",
    pages = "4171--4186",
    abstract = "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7{\%} (4.6{\%} absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement)."
}
@inproceedings{2019-sentence-bert,
    title = "Sentence-{BERT}: Sentence Embeddings using {S}iamese {BERT}-Networks",
    author = "Reimers, Nils  and
      Gurevych, Iryna",
    editor = "Inui, Kentaro  and
      Jiang, Jing  and
      Ng, Vincent  and
      Wan, Xiaojun",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D19-1410/",
    doi = "10.18653/v1/D19-1410",
    pages = "3982--3992",
    abstract = "BERT (Devlin et al., 2018) and RoBERTa (Liu et al., 2019) has set a new state-of-the-art performance on sentence-pair regression tasks like semantic textual similarity (STS). However, it requires that both sentences are fed into the network, which causes a massive computational overhead: Finding the most similar pair in a collection of 10,000 sentences requires about 50 million inference computations ({\textasciitilde}65 hours) with BERT. The construction of BERT makes it unsuitable for semantic similarity search as well as for unsupervised tasks like clustering. In this publication, we present Sentence-BERT (SBERT), a modification of the pretrained BERT network that use siamese and triplet network structures to derive semantically meaningful sentence embeddings that can be compared using cosine-similarity. This reduces the effort for finding the most similar pair from 65 hours with BERT / RoBERTa to about 5 seconds with SBERT, while maintaining the accuracy from BERT. We evaluate SBERT and SRoBERTa on common STS tasks and transfer learning tasks, where it outperforms other state-of-the-art sentence embeddings methods."
}

@INPROCEEDINGS{minhash,
  author={Broder, A.Z.},
  booktitle={Proceedings. Compression and Complexity of SEQUENCES 1997 (Cat. No.97TB100171)}, 
  title={On the resemblance and containment of documents}, 
  year={1997},
  volume={},
  number={},
  pages={21-29},
  keywords={Sampling methods;Web sites;Digital systems;Particle measurements;Fingerprint recognition;Explosions;Algorithm design and analysis;Clustering algorithms;Costs;Testing},
  doi={10.1109/SEQUEN.1997.666900}}

@inproceedings{lsh_classic,
author = {Charikar, Moses S.},
title = {Similarity estimation techniques from rounding algorithms},
year = {2002},
isbn = {1581134959},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/509907.509965},
doi = {10.1145/509907.509965},
abstract = {(MATH) A locality sensitive hashing scheme is a distribution on a family $F$ of hash functions operating on a collection of objects, such that for two objects x,y, PrhεF[h(x) = h(y)] = sim(x,y), where sim(x,y) ε [0,1] is some similarity function defined on the collection of objects. Such a scheme leads to a compact representation of objects so that similarity of objects can be estimated from their compact sketches, and also leads to efficient algorithms for approximate nearest neighbor search and clustering. Min-wise independent permutations provide an elegant construction of such a locality sensitive hashing scheme for a collection of subsets with the set similarity measure sim(A,B) = frac{|A ∩ B|}{|A ∪ B|}.(MATH) We show that rounding algorithms for LPs and SDPs used in the context of approximation algorithms can be viewed as locality sensitive hashing schemes for several interesting collections of objects. Based on this insight, we construct new locality sensitive hashing schemes for:A collection of vectors with the distance between → over u and → over v measured by \O{}(→ over u, → over v)/π, where \O{}(→ over u, → over v) is the angle between → over u) and → over v). This yields a sketching scheme for estimating the cosine similarity measure between two vectors, as well as a simple alternative to minwise independent permutations for estimating set similarity.A collection of distributions on n points in a metric space, with distance between distributions measured by the Earth Mover Distance (EMD), (a popular distance measure in graphics and vision). Our hash functions map distributions to points in the metric space such that, for distributions P and Q, EMD(P,Q) ≤ EhεF [d(h(P),h(Q))] ≤ O(log n log log n). EMD(P, Q).},
booktitle = {Proceedings of the Thiry-Fourth Annual ACM Symposium on Theory of Computing},
pages = {380–388},
numpages = {9},
location = {Montreal, Quebec, Canada},
series = {STOC '02}
}

@article{EMD,
  title={Approximation algorithms for classification problems with pairwise relationships: Metric labeling and Markov random fields},
  author={Kleinberg, Jon and Tardos, Eva},
  journal={Journal of the ACM (JACM)},
  volume={49},
  number={5},
  pages={616--639},
  year={2002},
  publisher={ACM New York, NY, USA}
}

@inproceedings{e2lsh,
  title={Locality-sensitive hashing scheme based on p-stable distributions},
  author={Datar, Mayur and Immorlica, Nicole and Indyk, Piotr and Mirrokni, Vahab S},
  booktitle={Proceedings of the twentieth annual symposium on Computational geometry},
  pages={253--262},
  year={2004}
}

@article{e2lsh_2,
  title={Approximate nearest neighbor: Towards removing the curse of dimensionality},
  author={Har-Peled, Sariel and Indyk, Piotr and Motwani, Rajeev},
  year={2012},
  publisher={Theory of Computing Exchange}
}

@inproceedings{c2lsh,
  title={Locality-sensitive hashing scheme based on dynamic collision counting},
  author={Gan, Junhao and Feng, Jianlin and Fang, Qiong and Ng, Wilfred},
  booktitle={Proceedings of the 2012 ACM SIGMOD international conference on management of data},
  pages={541--552},
  year={2012}
}

@inproceedings{lsb_forest,
  title={Quality and efficiency in high dimensional nearest neighbor search},
  author={Tao, Yufei and Yi, Ke and Sheng, Cheng and Kalnis, Panos},
  booktitle={Proceedings of the 2009 ACM SIGMOD International Conference on Management of data},
  pages={563--576},
  year={2009}
}

@misc{llsh,
      title={Can LSH (Locality-Sensitive Hashing) Be Replaced by Neural Network?}, 
      author={Renyang Liu and Jun Zhao and Xing Chu and Yu Liang and Wei Zhou and Jing He},
      year={2023},
      eprint={2310.09806},
      archivePrefix={arXiv},
      primaryClass={cs.IR},
      url={https://arxiv.org/abs/2310.09806}, 
}

@misc{neural_lsh_1,
      title={Learning Space Partitions for Nearest Neighbor Search}, 
      author={Yihe Dong and Piotr Indyk and Ilya Razenshteyn and Tal Wagner},
      year={2020},
      eprint={1901.08544},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1901.08544}, 
}
@article{qalsh,
  title={Query-aware locality-sensitive hashing for approximate nearest neighbor search},
  author={Huang, Qiang and Feng, Jianlin and Zhang, Yikai and Fang, Qiong and Ng, Wilfred},
  journal={Proceedings of the VLDB Endowment},
  volume={9},
  number={1},
  pages={1--12},
  year={2015},
  publisher={VLDB Endowment}
}

@inproceedings{iqalsh,
author = {Wang, Hongya and Sun, Cihai and Sun, Cihai},
title = {A Deep Learning Approach to Adaptive Query Processing for LSH},
year = {2022},
isbn = {9781450384964},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3556055.3556068},
doi = {10.1145/3556055.3556068},
abstract = {Approximate Nearest Neighbor(ANN) search has been attracting more attention in recent years. Locality-Sensitive Hashing (LSH) is one of the most promising methods to ANN problem because of its theoretical guarantees and nice empirical performance. It is important to select the appropriate number of hash functions m and collision filtering threshold l to get ANN of a query object. The appropriate parameters can reduce IO cost and improve query accuracy. However, the state-of-the-art LSH method, QALSH employs fixed with a given dataset D and an approximation ratio c. As for various query and different k for KNN problem, the does not change, which would lead to extra IO and decrease accuracy. Deep learning techniques have prominent capability to capture underlying data distributions including outliers. In this paper, we propose a novel method based on deep learning techniques, which can predict for different queries with given parameters including c, k and so on. Firstly, Variational auto-encoder(VAE) is used to transform the original query to a dense representation. Then the given parameters are also embeded to predict . In order to reflect the data distribution of the given dataset, we design a two-stage training strategy. Our experimental result shows the effectiveness and efficiency of our method.},
booktitle = {Proceedings of the 3rd International Conference on Industrial Control Network and System Engineering Research},
pages = {28–33},
numpages = {6},
keywords = {LSH, Deep learning, ANN},
location = {Nanjing, China},
series = {ICNSER '22}
}

@misc{llm_lsh,
      title={Neural Locality Sensitive Hashing for Entity Blocking}, 
      author={Runhui Wang and Luyang Kong and Yefan Tao and Andrew Borthwick and Davor Golac and Henrik Johnson and Shadie Hijazi and Dong Deng and Yongfeng Zhang},
      year={2024},
      eprint={2401.18064},
      archivePrefix={arXiv},
      primaryClass={cs.IR},
      url={https://arxiv.org/abs/2401.18064}, 
}

@misc{neurohash,
      title={NeuroHash: A Hyperdimensional Neuro-Symbolic Framework for Spatially-Aware Image Hashing and Retrieval}, 
      author={Sanggeon Yun and Ryozo Masukawa and SungHeon Jeong and Mohsen Imani},
      year={2024},
      eprint={2404.11025},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2404.11025}, 
}

@article{sift,
  title={Distinctive image features from scale-invariant keypoints},
  author={Lowe, David G},
  journal={International journal of computer vision},
  volume={60},
  number={2},
  pages={91--110},
  year={2004},
  publisher={Springer},
  doi={10.1023/B:VISI.0000029664.99615.94}
}

@article{mnist,
  title={Gradient-based learning applied to document recognition},
  author={LeCun, Yann and Bottou, Leon and Bengio, Yoshua and Haffner, Patrick},
  journal={Proceedings of the IEEE},
  volume={86},
  number={11},
  pages={2278--2324},
  year={1998},
  publisher={IEEE},
  doi={10.1109/5.726791}
}



@inproceedings{performance,
author = {Tr\"{u}mper, Lukas and Ben-Nun, Tal and Schaad, Philipp and Calotoiu, Alexandru and Hoefler, Torsten},
title = {Performance Embeddings: A Similarity-Based Transfer Tuning Approach to Performance Optimization},
year = {2023},
isbn = {9798400700569},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3577193.3593714},
doi = {10.1145/3577193.3593714},
abstract = {Performance optimization is an increasingly challenging but often repetitive task. While each platform has its quirks, the underlying code transformations rely on data movement and computational characteristics that recur across applications. This paper proposes to leverage those similarities by constructing an embedding space for subprograms. The continuous space captures both static and dynamic properties of loop nests via symbolic code analysis and performance profiling, respectively. Performance embeddings enable direct knowledge transfer of performance tuning between applications, which can result from autotuning or tailored improvements. We demonstrate this transfer tuning approach on case studies in deep neural networks, dense and sparse linear algebra compositions, and numerical weather prediction stencils. Transfer tuning reduces the search complexity by up to four orders of magnitude and outperforms the MKL library in sparse-dense matrix multiplication. The results exhibit clear correspondences between program characteristics and optimizations, outperforming prior specialized state-of-the-art approaches and generalizing beyond their capabilities.},
booktitle = {Proceedings of the 37th ACM International Conference on Supercomputing},
pages = {50–62},
numpages = {13},
keywords = {autotuning, performance optimization, peephole optimization, transfer tuning, embeddings, compilers},
location = {Orlando, FL, USA},
series = {ICS '23}
}

@INPROCEEDINGS{reposim,
  author={Li, Zihao and Filgueira, Rosa},
  booktitle={2023 IEEE 19th International Conference on e-Science (e-Science)}, 
  title={Mapping the Repository Landscape: Harnessing Similarity with RepoSim and RepoSnipy}, 
  year={2023},
  volume={},
  number={},
  pages={1-10},
  keywords={Deep learning;Codes;Semantic search;Source coding;Documentation;Syntactics;Software;semantic similarity;code search;code understanding;embeddings;pre-trained language models;GitHub},
  doi={10.1109/e-Science58273.2023.10254873}}

@INPROCEEDINGS{repo_similarity,
  author={Zhang, Honglin and Zhang, Leyu and Fang, Lei and Filgueira, Rosa},
  booktitle={2024 IEEE 20th International Conference on e-Science (e-Science)}, 
  title={Multi-Level AI-Driven Analysis of Software Repository Similarities}, 
  year={2024},
  volume={},
  number={},
  pages={1-10},
  keywords={Technological innovation;Analytical models;Codes;Navigation;Semantics;Documentation;Search engines;repository similarity;semantic analysis;repository clustering;code understanding;multi-level embeddings;pretrained language models;GitHub;mining software repositories},
  doi={10.1109/e-Science62913.2024.10678701}}

@INPROCEEDINGS{resume,
  author={Hourany, Jonathan and Zira, Aaron and Avas, Ignacio and Thiebaut, Nicolas},
  booktitle={2023 IEEE International Conference on Systems, Man, and Cybernetics (SMC)}, 
  title={Learning Résumé Embeddings with Search Data and Transformers}, 
  year={2023},
  volume={},
  number={},
  pages={2979-2983},
  keywords={Measurement;Self-supervised learning;Search engines;Transformers;Data models;Task analysis;Recruitment;Deep Learning;Recommender Systems;Machine Learning;Transfer Learning;Representation Learning;Natural Language Processing;Unsupervised Learning},
  doi={10.1109/SMC53992.2023.10394109}}
@article{reimers2019sentence,
  title={Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks},
  author={Reimers, N},
  journal={arXiv preprint arXiv:1908.10084},
  year={2019}
}
@ARTICLE{resume_2,
  author={Liu, Hao and Ge, Yong},
  journal={IEEE Transactions on Knowledge and Data Engineering}, 
  title={Job and Employee Embeddings: A Joint Deep Learning Approach}, 
  year={2023},
  volume={35},
  number={7},
  pages={7056-7067},
  keywords={Engineering profession;Companies;Task analysis;Data models;Context modeling;Transformers;Software;Career path;job embedding;employee embedding;job similarity search;employee similarity search},
  doi={10.1109/TKDE.2022.3180593}}

@article{yinhan2019roberta,
  title={RoBERTa: A robustly optimized BERT pretraining approach (2019)},
  author={Yinhan, Liu and Myle, Ott and Naman, Goyal and Jingfei, Du and Mandar, Joshi and Danqi, Chen and Omer, Levy and Mike, Lewis},
  journal={arXiv preprint arXiv:1907.11692},
  pages={1--13},
  year={2019},
  publisher={CoRR}
}

@article{liu2021combined,
  title={Combined embedding model for MiRNA-disease association prediction},
  author={Liu, Bailong and Zhu, Xiaoyan and Zhang, Lei and Liang, Zhizheng and Li, Zhengwei},
  journal={BMC bioinformatics},
  volume={22},
  pages={1--22},
  year={2021},
  publisher={Springer}
}

@ARTICLE{spoof,
  author={Li, Haoliang and Wang, Shiqi and He, Peisong and Rocha, Anderson},
  journal={IEEE Journal of Selected Topics in Signal Processing}, 
  title={Face Anti-Spoofing With Deep Neural Network Distillation}, 
  year={2020},
  volume={14},
  number={5},
  pages={933-946},
  keywords={Face;Training;Neural networks;Knowledge engineering;Feature extraction;Training data;Databases;Face anti-spoofing;deep learning;neural network distilling},
  doi={10.1109/JSTSP.2020.3001719}}

@misc{adam,
      title={Adam: A Method for Stochastic Optimization}, 
      author={Diederik P. Kingma and Jimmy Ba},
      year={2017},
      eprint={1412.6980},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1412.6980}, 
}

@inproceedings{bird2006nltk,
  title={NLTK: the natural language toolkit},
  author={Bird, Steven},
  booktitle={Proceedings of the COLING/ACL 2006 interactive presentation sessions},
  pages={69--72},
  year={2006}
}

@article{faiss,
  title={Billion-scale similarity search with GPUs},
  author={Johnson, Jeff and Douze, Matthijs and J{\'e}gou, Herv{\'e}},
  journal={IEEE Transactions on Big Data},
  volume={7},
  number={3},
  pages={535--547},
  year={2019},
  publisher={IEEE}
}

@article{jegou2011product,
  title={Product Quantization for Nearest Neighbor Search},
  author={J{\'e}gou, Herv{\'e} and Douze, Matthijs and Schmid, Cordelia},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume={33},
  number={1},
  pages={117--128},
  year={2011},
  doi={10.1109/TPAMI.2010.57}
}

@inproceedings{dong2011efficient,
  title={Efficient k-nearest neighbor graph construction for generic similarity measures},
  author={Dong, Wei and Moses, Charikar and Li, Kai},
  booktitle={Proceedings of the 20th International Conference on World Wide Web},
  pages={577--586},
  year={2011},
  doi={10.1145/1963405.1963487}
}

@article{burkhard1973some,
  title={Some approaches to best-match file searching},
  author={Burkhard, Walter A and Keller, Robert M},
  journal={Communications of the ACM},
  volume={16},
  number={4},
  pages={230--236},
  year={1973},
  doi={10.1145/362003.362012}
}

@article{levenshtein1966binary,
  title={Binary Codes Capable of Correcting Deletions, Insertions and Reversals},
  author={Levenshtein, Vladimir I.},
  journal={Soviet Physics Doklady},
  volume={10},
  number={8},
  pages={707--710},
  year={1966}
}

@article{navarro2001guided,
  title={A Guided Tour to Approximate String Matching},
  author={Navarro, Gonzalo},
  journal={ACM Computing Surveys},
  volume={33},
  number={1},
  pages={31--88},
  year={2001},
  doi={10.1145/375360.375365}
}

